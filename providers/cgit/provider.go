package cgit

import (
	"database/sql"
	"io"
	"strings"
	"sync"
	"time"

	"github.com/src-d/rovers/core"
	"github.com/src-d/rovers/providers"
	"github.com/src-d/rovers/providers/cgit/model"
	"github.com/src-d/rovers/utils"
	"github.com/src-d/rovers/utils/websearch"
	"github.com/src-d/rovers/utils/websearch/bing"

	"github.com/jpillora/backoff"
	"gopkg.in/inconshreveable/log15.v2"
	rmodel "gopkg.in/src-d/core-retrieval.v0/model"
	"gopkg.in/src-d/go-kallax.v1"
)

const (
	searchQuery = `"powered by cgit"||"generated by cgit"||"Commits per author per week"`

	providerName = "cgit"

	maxDurationToRetry = 16 * time.Second
	minDurationToRetry = 1 * time.Second
)

type provider struct {
	repositoriesStore   *model.RepositoryStore
	urlsStore           *model.URLStore
	scrapers            []*scraper
	searcher            websearch.Searcher
	backoff             *backoff.Backoff
	currentScraperIndex int
	mutex               *sync.Mutex
	lastPage            *page
}

func getBackoff() *backoff.Backoff {
	return &backoff.Backoff{
		Jitter: true,
		Factor: 2,
		Max:    maxDurationToRetry,
		Min:    minDurationToRetry,
	}
}

func NewProvider(bingKey string, DB *sql.DB) core.RepoProvider {
	p := &provider{
		repositoriesStore: model.NewRepositoryStore(DB),
		urlsStore:         model.NewURLStore(DB),
		scrapers:          []*scraper{},
		searcher:          bing.New(bingKey),
		backoff:           getBackoff(),
		mutex:             &sync.Mutex{},
	}

	return p
}

func (cp *provider) setCheckpoint(cgitUrl string, cgitPage *page) error {
	log15.Debug("adding new checkpoint url", "cgit URL", cgitUrl, "repository", cgitPage.RepositoryURL)

	return cp.repositoriesStore.Insert(
		&model.Repository{
			ID:      kallax.NewULID(),
			CgitURL: cgitUrl,
			URL:     cgitPage.RepositoryURL,
			Aliases: cgitPage.Aliases,
			HTML:    cgitPage.Html,
		})
}

func (cp *provider) alreadyProcessed(cgitUrl string, cgitPage *page) (bool, error) {
	c, err := cp.repositoriesStore.Count(
		model.NewRepositoryQuery().Where(
			kallax.And(
				kallax.Eq(model.Schema.Repository.CgitURL, cgitUrl),
				kallax.ArrayContains(model.Schema.Repository.Aliases,
					cgitPage.RepositoryURL),
			),
		),
	)

	return c > 0, err
}

func (cp *provider) getAllCgitUrlsAlreadyProcessed() ([]string, error) {
	rs, err := cp.urlsStore.Find(model.NewURLQuery())
	if err != nil {
		return nil, err
	}

	result := []string{}
	if err := rs.ForEach(func(cu *model.URL) error {
		result = append(result, cu.CgitUrl)

		return nil
	}); err != nil {
		return nil, err
	}

	return result, nil
}

func (cp *provider) saveNewCgitUrls(urls []string) error {
	for _, u := range urls {
		err := cp.urlsStore.Insert(&model.URL{CgitUrl: u, ID: kallax.NewULID()})
		switch {
		case err == nil:
			log15.Debug("New inserted cgit URL", "url", u)
			// TODO kallax: have a way to detect unique constraint violation
		case strings.Contains(err.Error(), "duplicate key value violates unique constraint"):
			log15.Debug("Duplicated cgit URL", "url", u)
		default:
			return err
		}
	}

	return nil
}

func (cp *provider) fillScrapers() {
	cgitUrlsSet := map[string]struct{}{}
	alreadyProcessedCgitUrls, err := cp.getAllCgitUrlsAlreadyProcessed()
	if err != nil {
		log15.Error("error getting cgit urls from database", "error", err)
	}

	possibleCgitUrls, err := cp.searcher.Search(searchQuery)
	if err != nil {
		log15.Error("error getting cgit urls from Searcher", "error", err)
	}

	mainCgitUrls := getAllMainCgitUrls(utils.URLsToStrings(possibleCgitUrls...))
	if err := cp.saveNewCgitUrls(mainCgitUrls); err != nil {
		log15.Error("Error saving new cgit urls", "missed cgit urls", mainCgitUrls, "error", err)
	}

	cp.joinUnique(cgitUrlsSet, mainCgitUrls, alreadyProcessedCgitUrls)
	for u := range cgitUrlsSet {
		log15.Info("adding new Scraper", "cgit URL", u)
		cp.scrapers = append(cp.scrapers, newScraper(u))
	}
}

func (cp *provider) joinUnique(set map[string]struct{}, slices ...[]string) {
	for _, slice := range slices {
		for _, e := range slice {
			set[e] = struct{}{}
		}
	}
}

func (cp *provider) Next() (*rmodel.Mention, error) {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()
	if cp.lastPage != nil {
		log15.Warn("some error happens when try to call Ack(), returning the last repository again",
			"repository", cp.lastPage.RepositoryURL)

		return cp.repositoryRaw(cp.lastPage), nil
	}

	if cp.isFirst() {
		cp.fillScrapers()
		if len(cp.scrapers) == 0 {
			log15.Warn("no scrapers found, sending an EOF because we have no data")
			return nil, io.EOF
		}
	}

	for {
		currentScraper := cp.scrapers[cp.currentScraperIndex]
		cgitUrl := currentScraper.URL
		repoData, err := currentScraper.Next()
		switch {
		case err == io.EOF:
			cp.nextScraper()
			if len(cp.scrapers) <= cp.currentScraperIndex {
				log15.Debug("all cgitUrls processed, ending provider iterator",
					"current index", cp.currentScraperIndex)
				cp.reset()
				return nil, io.EOF
			}
		case err != nil:
			log15.Error("error on scraper.next", "cgit URL", currentScraper.URL, "error", err)
			cp.handleRetries()
			return nil, err
		case err == nil:
			cp.backoff.Reset()
			processed, err := cp.alreadyProcessed(cgitUrl, repoData)
			if err != nil {
				return nil, err
			}

			if processed {
				log15.Debug("repository already processed", "cgit URL", cgitUrl, "url", repoData.RepositoryURL)
			} else {
				cp.lastPage = repoData
				return cp.repositoryRaw(repoData), nil
			}
		}
	}
}

func (cp *provider) handleRetries() {
	tts := cp.backoff.Duration()
	log15.Info("sleeping before next scraper request",
		"retries", cp.backoff.Attempt(), "time to sleep", tts)
	time.Sleep(tts)

	if tts >= maxDurationToRetry {
		log15.Warn("scraper request failed too many times. Skipping to the next scraper",
			"retries", cp.backoff.Attempt())
		cp.nextScraper()
	}
}

func (*provider) repositoryRaw(page *page) *rmodel.Mention {
	return &rmodel.Mention{
		Endpoint: page.RepositoryURL,
		Provider: providerName,
		VCS:      rmodel.GIT,
		Context:  providers.ContextBuilder{}.Aliases(page.Aliases),
	}
}

func (cp *provider) nextScraper() {
	cp.backoff.Reset()
	cp.currentScraperIndex++
}

func (cp *provider) isFirst() bool {
	return len(cp.scrapers) == 0
}

func (cp *provider) reset() {
	cp.scrapers = []*scraper{}
	cp.currentScraperIndex = 0
}

func (cp *provider) Ack(err error) error {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()
	if err == nil && cp.lastPage != nil {
		err = cp.setCheckpoint(cp.scrapers[cp.currentScraperIndex].URL, cp.lastPage)
		if err != nil {
			return err
		} else {
			cp.lastPage = nil
		}
	}

	return nil
}

func (cp *provider) Close() error {
	return nil
}

func (cp *provider) Name() string {
	return providerName
}
